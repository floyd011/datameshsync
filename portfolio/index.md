---
layout: default
title: "Portfolio of Milorad Spasić"
---
# Welcome to the Portfolio of Milorad Spasić

[Home](#welcome-to-the-portfolio-of-milorad-spasić) &nbsp; &nbsp; [Skills](#skills) &nbsp; &nbsp;[Projects](#projects) &nbsp; &nbsp;[Experience](#experience) &nbsp; &nbsp;[Certifications](#certifications) &nbsp; &nbsp;[Contact](#contact)

## AI/LLM Engineer & Solution Architect – Data & AI Integration Specialist

Experienced AI Engineer specializing in Large Language Model (LLM) development and deployment with strong expertise in transformer architectures, fine-tuning techniques (LoRA, QLoRA), and Retrieval-Augmented Generation (RAG) systems.

Proficient in building end-to-end AI pipelines, including data ingestion, embedding generation, vector search using Qdrant, and RESTful API integration via FastAPI and Streamlit. Skilled in prompt engineering and developing multi-agent orchestration systems using LangChain for complex, knowledge-driven workflows.

Hands-on experience with state-of-the-art models such as LLaMA3, Gemma, and OpenAI APIs, optimized for local deployment with GGUF format and Ollama runtime. Comfortable working with PyTorch, HuggingFace transformers, and AI frameworks for training, evaluation, and inference.

Focused on scalable, production-ready AI solutions that integrate intelligent document processing, semantic search, and dynamic response generation.

A seasoned Data Engineer with two decades of experience in designing, optimizing, and managing high-performance data systems. The focus is on developing reliable data pipelines, architecting scalable and resilient databases, and deploying cloud-native solutions to enable real-time analytics and enterprise-level data processing.

### Core Expertise

-	**LLM Engineering** & Prompt Design: Practical experience with OpenAI, LLaMA3, Gemma, and local model deployment via Ollama. Skilled in designing efficient prompts and dynamic AI workflows for document analysis, search, and decision-making tasks.
  
-	**RAG Systems & Vector Search**: Built custom Retrieval-Augmented Generation pipelines using Qdrant and various embedding models. Developed FastAPI-based services that perform real-time document querying, intelligent tagging (via KeyBERT), and semantic search.
  
-	**Model Fine-Tuning & Optimization**: Worked with LoRA, QLoRA, and GGUF formats to fine-tune and serve models locally. Created scripts to train and evaluate models in constrained environments, including exporting for use with Ollama.
  
-	**HPC Infrastructure Management** - Administration of supercomputer clusters, DGX nodes, GPU/CPU resources, parallel file systems, and interconnects (e.g. Infiniband).
  
-	**AI Agents & Multi-Agent Systems**: Designed LangChain-based multi-agent pipelines for task decomposition, coordination, and reasoning. Focused on agent orchestration for knowledge-intensive tasks.
  
- **NLP & Transformers**: Experience with tokenization, embeddings, and transformer architecture (HuggingFace, PyTorch). Able to train and evaluate models for intelligent document processing and NLP automation tasks.
  
- **Deployment & APIs**: Built RESTful APIs and Streamlit apps to integrate LLMs into end-user applications. Familiar with scalable deployment using FastAPI and real-time interaction with AI services.
 
- **Data Engineering & Architecture:**  
  Proficiency in Python and SQL, with extensive experience in ETL/ELT frameworks including Debezium, Kafka, Greenplum, and Hadoop, enabling robust real-time data streaming and analytics.

- **Database Administration & Optimization:**  
  Over 20 years of experience managing SQL Server, PostgreSQL, and distributed databases, with deep expertise in query optimization, high availability configurations, and disaster recovery strategies.

- **DevOps & Cloud Solutions:**  
  Expertise in Docker, Kubernetes, Terraform, Ansible, and cloud platforms such as AWS.

- **Microservices & Automation:**  
  Development of containerized, event-driven microservices and adoption of GitOps workflows for building scalable and resilient data platforms.

- **Performance & Security:**  
  Advanced knowledge of indexing techniques, data partitioning, replication, and compliance with regulatory standards such as GDPR.

### Professional Impact

- **Building local/private RAG systems**
- **Developing lightweight, adaptable AI agents**
- **Integrating LLMs into intelligent document workflows**
- **Optimizing models for on-device or serverless use**

- Designed and implemented real-time analytics platforms capable of processing millions of events daily using technologies like Kafka, Python, and Kubernetes.  
- Optimized high-throughput transactional databases for enterprise clients, achieving up to 60% reduction in query execution times and improved system resilience.  
- Led seamless migrations from Microsoft SQL Server to PostgreSQL, ensuring minimal downtime and data integrity throughout the transition.  
- Architected cloud-based data solutions that enhanced scalability, operational efficiency, and cost-effectiveness.
- Solution Architect with over 10 years of experience in data engineering and architecture, delivering end-to-end solutions that support real-time analytics, big data processing, and seamless cloud/on-premise integration. Formerly a Senior Data Engineer, now responsible for defining high-level data strategies, designing reference architectures, and guiding engineering teams in implementing secure and maintainable systems.

This portfolio showcases a blend of cloud-native development on AWS and hybrid integration with on-premise infrastructure, emphasizing performance, data quality, and long-term maintainability.

---

# Skills

[Home](#welcome-to-the-portfolio-of-milorad-spasić) &nbsp; &nbsp; [Skills](#skills) &nbsp; &nbsp;[Projects](#projects) &nbsp; &nbsp;[Experience](#experience) &nbsp; &nbsp;[Certifications](#certifications) &nbsp; &nbsp;[Contact](#contact)

## Technical Skills

## AI/LLM engineer & Data/AI integrator

- Building local/private RAG systems
- Developing lightweight, adaptable AI agents
- Integrating LLMs into intelligent document workflows
- Optimizing models for on-device or serverless use
	
### Data Integration Tools
- Debezium, Kafka  

### Databases
- Microsoft SQL Server, MySQL, Oracle, PostgreSQL, Vertica, Greenplum  

### Programming Languages
- Python (data processing and pipeline orchestration), SQL, Bash  

### Containerization & Orchestration
- Docker, Kubernetes

### Infrastructure as Code
- Ansible, Terraform, Pulumi  

### Monitoring & Logging
- Prometheus, Grafana, Graylog  

### Data Architecture
- Event-driven architecture, Microservices, Real-time data processing pipelines  

## Data Engineer

10+ years of experience in data engineering, with a strong emphasis on building robust data pipelines using Python for automation and scripting, combined with advanced SQL skills for querying, optimizing, and managing relational databases.
Designing and implementing modern data architectures, with a strong focus on scalability, performance, and availability. This includes expertise in key architectural components such as cloud storage, distributed computing frameworks, and streaming or batch data processing pipelines to enable robust, flexible, and efficient data solutions.

## Database architect and database engineer/developer 

25+ years of experience in Administrating and working mostly on highly transactional relational databases (SQL Server, PostgreSQL).
- Proficiency in ETL/ELT data pipeline with Debezium and Kafka to analytic databases Greenplum, Vertica, Hadoop.
- Proficiency programming in Python.

## Database Management Systems (DBMS)

- Microsoft SQL Server (2000 - Latest Version)
- PostgreSQL (9.6 - Latest Version)

## Database Development & Optimization

- Complex stored procedures, functions, triggers, and views
- Advanced indexing strategies (Clustered, Non-Clustered, Full-Text, JSON/GIN Indexes)
- Query tuning and optimization (Execution Plans, Query Store, Hints, Statistics)
- Partitioning strategies for large datasets
- Columnstore and in-memory OLTP optimization

## Database Programming & Query Languages

- **T-SQL** (Microsoft SQL Server)
- **PL/SQL** (Oracle)
- **PL/PGSQL** (PostgreSQL)

## High Availability & Disaster Recovery

- Always On Availability Groups (SQL Server)
- Log shipping, database mirroring, and replication
- Failover Clustering (Windows and PostgreSQL)
- PITR (Point-in-Time Recovery) and WAL archiving (PostgreSQL)
- Backup strategies (Full, Differential, Incremental, PITR, Snapshots)

## Security & Compliance

- Role-based access control (RBAC) and fine-grained permissions
- Transparent Data Encryption (TDE) and row-level security
- Dynamic Data Masking & Always Encrypted (SQL Server)
- Data auditing, GDPR 
- Secure database configurations and vulnerability assessments

## ETL & Data Integration

- SQL Server Integration Services (SSIS)
- PostgreSQL Foreign Data Wrappers (FDW)
- Data migration & transformation (ETL pipelines, CDC, bulk inserts)
- JSON, XML, and CSV data handling

## Infrastructure & Automation

- Scripting automation with Python and Bash
- Database infrastructure as code (IaC) using Terraform
- Containerization with Docker for PostgreSQL
- Cloud database management (Azure SQL, Amazon RDS, Google Cloud SQL)
- Database monitoring with Prometheus, Grafana, and SQL Server Profiler

----

## Certifications

[Home](#welcome-to-the-portfolio-of-milorad-spasić) &nbsp; &nbsp; [Skills](#skills) &nbsp; &nbsp;[Projects](#projects) &nbsp; &nbsp;[Experience](#experience) &nbsp; &nbsp;[Certifications](#certifications) &nbsp; &nbsp;[Contact](#contact)

- HP ASE – Vertica Big Data Solutions Administrator  
- Pivotal Greenplum Database Administrator  
- Pivotal Greenplum Database Developer  
- Microsoft Certified Solutions Expert in Data Management and Analytics  

---

# Projects

[Home](#welcome-to-the-portfolio-of-milorad-spasić) &nbsp; &nbsp; [Skills](#skills) &nbsp; &nbsp;[Projects](#projects) &nbsp; &nbsp;[Experience](#experience) &nbsp; &nbsp;[Certifications](#certifications) &nbsp; &nbsp;[Contact](#contact)

## Project Highlights

### Private RAG Pipeline with FastAPI, Qdrant, and Fine-Tuned LLaMA

**Document Ingestion (PDF Import Module)**:

- Implemented a PDF ingestion pipeline that accepts multiple documents through a drag-and-drop UI (Streamlit) or REST API (FastAPI).
- Each PDF is parsed using PyMuPDF or pdfplumber to extract clean, structured text.
- Metadata (e.g., title, source, page count) is captured for later filtering and display.

**Embedding Generation & Storage**:

- Extracted text chunks (based on token limits) are passed to a selected embedding model (e.g., all-MiniLM-L6-v2 or bge-base-en) using HuggingFace or SentenceTransformers.
- Each chunk is vectorized and stored in Qdrant, a high-speed vector search engine, along with its metadata and original content.
- The vector index is organized by document and optionally namespace for multi-tenant support.

**Predictive Tagging (Keyword Extraction & Classification)**:

- For each chunk, we used KeyBERT or a transformer-based classifier to extract semantic tags.
- Tags are added to the Qdrant metadata fields for advanced filtering and smart querying (e.g., “find security-related documents tagged with ‘OAuth’”).
- An option for human-in-the-loop review of auto-tags is included in the Streamlit UI.

**Backend Search API (FastAPI with RAG Logic)**:

- A RESTful API is built using FastAPI, exposing endpoints for:
- Document upload and indexing
- Vector similarity search with hybrid (tag + semantic) filtering
- Prompt-based document querying (RAG)
- API handles chunked retrieval from Qdrant based on user queries, re-ranks results, and composes context windows for LLM input.

**Local LLM Integration (LLaMA + Ollama)**:

- Deployed a fine-tuned version of LLaMA 3 using LoRA/QLoRA for specialization in   documentation-based search tasks.
- The model is served locally via Ollama, exposing a simple HTTP API.
- Input prompt includes top-k document chunks retrieved from Qdrant + user query → model generates the final answer or summary.
- Training used a custom instruction dataset based on the ingested PDF corpus.

**Streamlit Frontend UI**:

- Developed a clean, user-friendly interface to:
- Upload and preview PDF documents
- View automatically extracted tags
- Enter search queries and view generated answers
- Inspect which chunks were retrieved and used in the generation
- UI is directly connected to FastAPI backend via HTTP calls.
  
### Real-Time Analytics Platform

- **Objective:**  
  Developed a real-time analytics platform to process and analyze data for business intelligence applications.

- **Technologies Used:**  
  Debezium, Kafka, Python, Greenplum, Docker, Kubernetes, Graylog, Prometheus, Grafana

- **Key Achievements:**  
  - Achieved sub-second latency in capturing and processing database changes  
  - Designed and implemented scalable microservices capable of handling millions of events daily  
  - Built Grafana dashboards to monitor system performance and critical operational metrics  

### Microservices-Based Data Pipeline

- **Objective:**  
  Developed a microservices-based architecture to enable real-time streaming and processing of large datasets from multiple relational databases.

- **Technologies Used:**  
  Debezium, Kafka, Python, Docker, Kubernetes

- **Key Achievements:**  
  - Automated deployment and scaling of services using Kubernetes  
  - Reduced operational complexity by centralizing logs through Graylog containers  
  - Improved fault tolerance via Kafka partitioning and replication strategies  

### GPS Tracking & Telemetry Data Pipeline

- Designed and maintained high-performance, real-time data pipelines focused on GPS tracking systems and telemetry data ingestion.  
- Engineered solutions capable of processing hundreds of millions of records per day to support real-time analytics.  
- Leveraged cloud infrastructure, containerization, and data streaming technologies to optimize workflows for high-velocity, high-volume environments.  

---

# Experience

[Home](#welcome-to-the-portfolio-of-milorad-spasić) &nbsp; &nbsp; [Skills](#skills) &nbsp; &nbsp;[Projects](#projects) &nbsp; &nbsp;[Experience](#experience) &nbsp; &nbsp;[Certifications](#certifications) &nbsp; &nbsp;[Contact](#contact)

## Professional Experience

### AI Agents & Multi-Agent Systems

-Designed LangChain-based multi-agent pipelines for task decomposition, coordination, and reasoning. Focused on agent orchestration for knowledge-intensive tasks.
  
### NLP & Transformers
  
- Experience with tokenization, embeddings, and transformer architecture (HuggingFace, PyTorch). Able to train and evaluate models for intelligent document processing and NLP automation tasks.
- 
### Local LLM Integration (LLaMA + Ollama)

- Deployed a fine-tuned version of LLaMA 3 using LoRA/QLoRA for specialization in   documentation-based search tasks.
- The model is served locally via Ollama, exposing a simple HTTP API.
- Input prompt includes top-k document chunks retrieved from Qdrant + user query → model generates the final answer or summary.
- Training used a custom instruction dataset based on the ingested PDF corpus.

### 1. Database Change Data Capture (CDC) with Debezium

- Implemented Change Data Capture (CDC) pipelines using Debezium to capture real-time changes from relational databases such as Microsoft SQL Server, MySQL, Oracle, and PostgreSQL.  
- Configured Debezium connectors to ensure minimal latency and high reliability in data streaming processes.  

### 2. Data Streaming with Kafka

- Designed and managed Kafka topics to support high-throughput, real-time data ingestion.  
- Ensured data consistency and fault tolerance through the implementation of robust Kafka configurations.  

### 3. Data Processing and Loading

- Developed Python-based consumers for processing Kafka streams and loading enriched data into the Greenplum analytical database.  
- Applied transformation and enrichment logic to prepare data for analytical use cases.  

### 4. Microservices Architecture

- Designed and developed modular, scalable microservices for data ingestion, transformation, and delivery.  
- Deployed microservices within a containerized environment using Docker.  
- Orchestrated containers with Kubernetes to maintain high availability and scalability of data workflows.  

### 5. Logging and Monitoring

- Centralized application logging using Logstash containers to enable real-time log tracking and diagnostics.  
- Monitored system health and performance metrics via Prometheus, with dashboards and visualizations created in Grafana.  

### 6. Data Analytics Pipeline

- Designed and optimized end-to-end pipelines capable of handling high-volume, real-time analytical workloads.  
- Ensured seamless integration between source systems, streaming platforms, and the Greenplum analytical database.

---

# Contact

[Home](#welcome-to-the-portfolio-of-milorad-spasić) &nbsp; &nbsp; [Skills](#skills) &nbsp; &nbsp;[Projects](#projects) &nbsp; &nbsp;[Experience](#experience) &nbsp; &nbsp;[Certifications](#certifications) &nbsp; &nbsp;[Contact](#contact)

  - email: [Milorad Spasic](mailto:engine@datameshsync.info)
  - Linkedin: [Linkedin](https://www.linkedin.com/in/milorad-spasic)
